{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Parallel Computing in fmdtools\n",
    "\n",
    "This notebook will discuss how to use parallel programming in fmdtools, including:\n",
    "- how to set up a model for parallelism\n",
    "- syntax for using parallelism in simulation functions\n",
    "- considerations for optimizing computational performance in a model\n",
    "\n",
    "```\n",
    "Copyright © 2024, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.\n",
    "\n",
    "The “\"Fault Model Design tools - fmdtools version 2\"” software is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. \n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T23:54:36.594994Z",
     "iopub.status.busy": "2024-10-21T23:54:36.594483Z",
     "iopub.status.idle": "2024-10-21T23:54:37.214624Z",
     "shell.execute_reply": "2024-10-21T23:54:37.213642Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ex_pump'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex_pump\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfmdtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaultDomain, FaultSample\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfmdtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpropagate\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpropagate\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ex_pump'"
     ]
    }
   ],
   "source": [
    "from ex_pump import * \n",
    "from fmdtools.sim.sample import FaultDomain, FaultSample\n",
    "import fmdtools.sim.propagate as propagate\n",
    "import fmdtools.analyze as an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the pump example (see `ex_pump.py`) to illustrate the use of parallelism in fmdtools. This is fairly simple model, and thus it should be noted that there may be considerations with more complex models which may not be adequately covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:28.812329Z",
     "iopub.status.busy": "2024-08-22T16:08:28.812329Z",
     "iopub.status.idle": "2024-08-22T16:08:28.823012Z",
     "shell.execute_reply": "2024-08-22T16:08:28.823012Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl = Pump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:28.824514Z",
     "iopub.status.busy": "2024-08-22T16:08:28.824514Z",
     "iopub.status.idle": "2024-08-22T16:08:28.892112Z",
     "shell.execute_reply": "2024-08-22T16:08:28.892112Z"
    }
   },
   "outputs": [],
   "source": [
    "result, mdlhist = propagate.nominal(mdl, desired_result='graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:28.892112Z",
     "iopub.status.busy": "2024-08-22T16:08:28.892112Z",
     "iopub.status.idle": "2024-08-22T16:08:29.152519Z",
     "shell.execute_reply": "2024-08-22T16:08:29.152519Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = result.graph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:29.152519Z",
     "iopub.status.busy": "2024-08-22T16:08:29.152519Z",
     "iopub.status.idle": "2024-08-22T16:08:29.300891Z",
     "shell.execute_reply": "2024-08-22T16:08:29.300891Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = mdlhist.plot_line('flows.ee_1.s.current', 'flows.wat_2.s.flowrate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checks\n",
    "\n",
    "Before attempting to leverage parallelism in model execution, it can be helpful to check whether a model is compatible with python parallel computing libraries. In order for a model to be parallelized, it must be compatible with [pickling](https://docs.python.org/3/library/pickle.html#:~:text=%E2%80%9CPickling%E2%80%9D%20is%20the%20process%20whereby,back%20into%20an%20object%20hierarchy.)--python's method of data serialization. This is used in parallel programming methods to copy the model from the main process thread to the seperate processes of the pool.\n",
    "\n",
    "fmdtools has two methods to check whether a model can be pickled, `check_pickleability` and `check_model_pickleability`. The main difference between these is that `check_pickleability` works for all objects (e.g. functions and flows), while `check_model_pickleability` gives more information for an overall model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:29.300891Z",
     "iopub.status.busy": "2024-08-22T16:08:29.300891Z",
     "iopub.status.idle": "2024-08-22T16:08:29.307168Z",
     "shell.execute_reply": "2024-08-22T16:08:29.307168Z"
    }
   },
   "outputs": [],
   "source": [
    "from fmdtools.define.object.base import check_pickleability\n",
    "from fmdtools.define.architecture.base import check_model_pickleability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:29.307168Z",
     "iopub.status.busy": "2024-08-22T16:08:29.307168Z",
     "iopub.status.idle": "2024-08-22T16:08:29.314344Z",
     "shell.execute_reply": "2024-08-22T16:08:29.314344Z"
    }
   },
   "outputs": [],
   "source": [
    "unpickleable_attributes = check_pickleability(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:29.314344Z",
     "iopub.status.busy": "2024-08-22T16:08:29.314344Z",
     "iopub.status.idle": "2024-08-22T16:08:31.132769Z",
     "shell.execute_reply": "2024-08-22T16:08:31.132769Z"
    }
   },
   "outputs": [],
   "source": [
    "check_model_pickleability(mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model is pickleable. However, this may not be the case for all structures if they rely on unpickleable data structures, one common one being iterators like .values()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Parallelism in Simulation\n",
    "\n",
    "Parallelism generally requires using some external parallel processing toolkit. The syntax used by fmdtools methods is compatible with:\n",
    "- [multiprocessing](https://docs.python.org/3/library/multiprocessing.html), python's default parallel computing library\n",
    "- [multiprocess](https://pypi.org/project/multiprocess/), a fork of multiprocessing developed by The UQ Foundation\n",
    "- [pathos](https://github.com/uqfoundation/pathos), a broader parallel computing package developed by The UQ Foundation\n",
    "\n",
    "And any other package that emulates multiprocessing.Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:31.132769Z",
     "iopub.status.busy": "2024-08-22T16:08:31.132769Z",
     "iopub.status.idle": "2024-08-22T16:08:31.156902Z",
     "shell.execute_reply": "2024-08-22T16:08:31.156902Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import multiprocess as ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelism can speed up simulation time when there is a large number of independent simulations to run. The prefered methods for using parallelism are to use a `NominalApproach` or `SampleApproach` with the methods:\n",
    "- propagate.singlefaults (for all single-fault scenarios in a static model with no approach)\n",
    "- propagate.approach (for sampling a set of faults)\n",
    "- propagate.nominal_approach (for simulating the model nominally over a set of parameters)\n",
    "- propagate.nested_approach (for sampling a set of faults over a set of model parameters)\n",
    "\n",
    "These methods can be run in parallel by sending them a `pool` object from one of these modules as the optional `pool` argument. Further details on setting up and running an approach are provided in `docs/Approach Use-Cases.ipynb` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:31.156902Z",
     "iopub.status.busy": "2024-08-22T16:08:31.156902Z",
     "iopub.status.idle": "2024-08-22T16:08:33.806925Z",
     "shell.execute_reply": "2024-08-22T16:08:33.806800Z"
    }
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(4)\n",
    "fd = FaultDomain(mdl)\n",
    "fd.add_all()\n",
    "fs = FaultSample(fd)\n",
    "fs.add_fault_phases()\n",
    "endclasses, mdlhists = propagate.fault_sample(mdl, fs, pool=pool)\n",
    "#an.tabulate.simplefmea(endclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:33.806925Z",
     "iopub.status.busy": "2024-08-22T16:08:33.806925Z",
     "iopub.status.idle": "2024-08-22T16:08:33.813962Z",
     "shell.execute_reply": "2024-08-22T16:08:33.813962Z"
    }
   },
   "outputs": [],
   "source": [
    "pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it helps to \"warm up\" the pool. See, for example, how much longer per-second iterations take for the above compared to below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:33.815115Z",
     "iopub.status.busy": "2024-08-22T16:08:33.815115Z",
     "iopub.status.idle": "2024-08-22T16:08:36.206633Z",
     "shell.execute_reply": "2024-08-22T16:08:36.206633Z"
    }
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(4)\n",
    "endclasses, mdlhists = propagate.single_faults(mdl, pool=pool)\n",
    "#an.tabulate.simplefmea(endclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be helpful to verify that the results of parallel simulation and normal serial execution are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:36.206633Z",
     "iopub.status.busy": "2024-08-22T16:08:36.206633Z",
     "iopub.status.idle": "2024-08-22T16:08:39.120754Z",
     "shell.execute_reply": "2024-08-22T16:08:39.120754Z"
    }
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(4)\n",
    "endclasses_par, mdlhists = propagate.single_faults(mdl, pool=pool, close_pool=False)\n",
    "#tab_par = an.tabulate.simplefmea(endclasses_par)\n",
    "endclasses, mdlhists = propagate.single_faults(mdl)\n",
    "#tab = an.tabulate.simplefmea(endclasses)\n",
    "#tab - tab_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:39.120754Z",
     "iopub.status.busy": "2024-08-22T16:08:39.120754Z",
     "iopub.status.idle": "2024-08-22T16:08:41.111667Z",
     "shell.execute_reply": "2024-08-22T16:08:41.111667Z"
    }
   },
   "outputs": [],
   "source": [
    "# pool = mp.Pool(4)\n",
    "endclasses_par, mdlhists = propagate.fault_sample(mdl, fs, pool=pool)\n",
    "#tab_par = an.tabulate.simplefmea(endclasses_par)\n",
    "endclasses, mdlhists = propagate.fault_sample(mdl, fs)\n",
    "#tab = an.tabulate.simplefmea(endclasses)\n",
    "#tab - tab_par\n",
    "pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While fmdtools built-in methods are the easiest way to leverage parallelism, it can also be used with custom arguments/methods to meet the needs of simulation. However, (on Windows) these methods need to be defined in an external module with an \"if __name__=='__main__':\" statement, otherwise execution will hang from spawning new processes. This has to do with how multiprocessing works in windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how parellism can be leveraged manually for a desired use-case, below the model is run over the blockage fault mode at time t=1 with a different model parameter (delayed failure behavior), as defined in the `parallelism_methods.py` module in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:41.111667Z",
     "iopub.status.busy": "2024-08-22T16:08:41.111667Z",
     "iopub.status.idle": "2024-08-22T16:08:41.121141Z",
     "shell.execute_reply": "2024-08-22T16:08:41.121141Z"
    }
   },
   "outputs": [],
   "source": [
    "from parallelism_methods import delay_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:41.121141Z",
     "iopub.status.busy": "2024-08-22T16:08:41.121141Z",
     "iopub.status.idle": "2024-08-22T16:08:43.807747Z",
     "shell.execute_reply": "2024-08-22T16:08:43.807747Z"
    }
   },
   "outputs": [],
   "source": [
    "results = delay_test()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, the model is run many times over a given fault with different delay parameters. It should be noted that this approach is not especially efficient, since the nominal scenario is simulated at each call of `propagate.one_fault()`. It is thus preferred to use the appropriate fault/parameter sampling approaches and propagate methods, since these methods only run the nominal simulation once for fault scenarios and can also use staged execution (copying the model at fault time for fault scenarios) to reduce the cost of each simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Parallelism is often used in computation to speed up up a set of independent simulations. Conventionally, one might say it leads to a reduced computational cost of $t/n$, where t was the original time of the set of processes, and n is the number of cores.\n",
    "\n",
    "However, this computational performance increase is dependent on the implementation. In Python, there is some overhead from \n",
    "from communicating data structures in and out of parallel threads which can become a significant consideration when the data structures are large. Additionally, different Pools can execute more or less efficiently. Below these are each compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:43.807747Z",
     "iopub.status.busy": "2024-08-22T16:08:43.807747Z",
     "iopub.status.idle": "2024-08-22T16:08:43.814220Z",
     "shell.execute_reply": "2024-08-22T16:08:43.814220Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from parallelism_methods import compare_pools, instantiate_pools, terminate_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:43.814220Z",
     "iopub.status.busy": "2024-08-22T16:08:43.814220Z",
     "iopub.status.idle": "2024-08-22T16:08:43.880296Z",
     "shell.execute_reply": "2024-08-22T16:08:43.880296Z"
    }
   },
   "outputs": [],
   "source": [
    "cores=4\n",
    "pools = instantiate_pools(cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the baseline comparison, where the the following parameters characterize the sampling approach:\n",
    " - single faults: only the single-fault scenarios are considered\n",
    " - 3 points per phase: an evenly-spaced quadrature is sampled at each phase of operation (start, on, end) for the model\n",
    " - staged: the model is copied at each point in time where faults is injected during the model time to save computation\n",
    " - track: the entire model history is returned for each simulation\n",
    " \n",
    " This is typical for a small model like this where the per-model expense is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:43.880296Z",
     "iopub.status.busy": "2024-08-22T16:08:43.880296Z",
     "iopub.status.idle": "2024-08-22T16:08:53.662332Z",
     "shell.execute_reply": "2024-08-22T16:08:53.662332Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(track='all')\n",
    "fs = FaultSample(fd)\n",
    "fs.add_fault_phases(args = (3,))\n",
    "\n",
    "pools = instantiate_pools(5)\n",
    "_ = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:53.662332Z",
     "iopub.status.busy": "2024-08-22T16:08:53.662332Z",
     "iopub.status.idle": "2024-08-22T16:08:58.651716Z",
     "shell.execute_reply": "2024-08-22T16:08:58.651716Z"
    }
   },
   "outputs": [],
   "source": [
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)\n",
    "exectimes_baseline = exectimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:58.651716Z",
     "iopub.status.busy": "2024-08-22T16:08:58.651716Z",
     "iopub.status.idle": "2024-08-22T16:08:58.755985Z",
     "shell.execute_reply": "2024-08-22T16:08:58.755985Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center')\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Baseline Performance - Some faults, Staged, Normal Simulation, Full Model History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, in this situation, both the multiprocessing and threadpool pools give computational performance increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: No Histories\n",
    "\n",
    "In the below comparison, the same simulation approach is run, except without tracking a history of model states through the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:08:58.755985Z",
     "iopub.status.busy": "2024-08-22T16:08:58.755985Z",
     "iopub.status.idle": "2024-08-22T16:09:02.712672Z",
     "shell.execute_reply": "2024-08-22T16:09:02.712672Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(track='none')\n",
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:02.712672Z",
     "iopub.status.busy": "2024-08-22T16:09:02.712672Z",
     "iopub.status.idle": "2024-08-22T16:09:02.876952Z",
     "shell.execute_reply": "2024-08-22T16:09:02.876952Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.5, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Many Faults, Staged, Normal Simulation, No Model History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, in this situation, the overall simulation expense decreases, even in the serial execution case.\n",
    "\n",
    "Additionally, the case for using a parallel processing pool increases somewhat. This is because passing the model history back to the main process is nearly comparable in time to simulation itself.\n",
    "\n",
    "As a result, removing it saves a large amount of computational time when using parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Many Faults\n",
    "\n",
    "In the below comparison, many faults are injected in the system to increase the number of scenarios (ostensibly making the case better for parallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:02.876952Z",
     "iopub.status.busy": "2024-08-22T16:09:02.876952Z",
     "iopub.status.idle": "2024-08-22T16:09:12.566430Z",
     "shell.execute_reply": "2024-08-22T16:09:12.566430Z"
    }
   },
   "outputs": [],
   "source": [
    "fs_many = FaultSample(fd)\n",
    "fs_many.add_fault_phases(args = (7,))\n",
    "mdl=Pump(track='all')\n",
    "exectimes = compare_pools(mdl, fs_many, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:12.566430Z",
     "iopub.status.busy": "2024-08-22T16:09:12.566430Z",
     "iopub.status.idle": "2024-08-22T16:09:12.691423Z",
     "shell.execute_reply": "2024-08-22T16:09:12.691423Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.8, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Many Faults, Staged, Normal Simulation, Full Model History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, increasing the number of joint-fault scenarios increases computational costs significantly--as would be expected.\n",
    "\n",
    "In this situation, multiprocessing performs comparatively better, but only slightly--instead of taking 1/4 the time, it only takes about 1/2 the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Long simulation\n",
    "\n",
    "It may be of interest to simulate how the comparative performance changes for longer simulations. In this comparison, the simulation time is extended tenfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:12.691423Z",
     "iopub.status.busy": "2024-08-22T16:09:12.691423Z",
     "iopub.status.idle": "2024-08-22T16:09:17.729803Z",
     "shell.execute_reply": "2024-08-22T16:09:17.729803Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(sp=dict(times=(0,20, 500)), track='all')\n",
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:17.731349Z",
     "iopub.status.busy": "2024-08-22T16:09:17.731349Z",
     "iopub.status.idle": "2024-08-22T16:09:17.855547Z",
     "shell.execute_reply": "2024-08-22T16:09:17.855547Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.8, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Normal Faults, Staged, Long Simulation, Full Model History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the simulation time does increase significantly--about tenfold. In terms of comparative performance, pools other than multiprocessing now become competitive, though multiprocessing is still the fastest overall. \n",
    "\n",
    "This shows the main case for using parallesism--speeding up long simulations. Short simulations unfortunately require a significant amount of overhead due to copying in and out of the individual thread and we thus see less of a case for them there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Long Simulation No Tracking\n",
    "\n",
    "Finally, it may be interesting to see how performance is affected in long simulations when there is no tracking. This is because in these simulations, there should be very little overhead from creating the respective data structures, even when there is a long simulation. This comparison is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:17.855547Z",
     "iopub.status.busy": "2024-08-22T16:09:17.855547Z",
     "iopub.status.idle": "2024-08-22T16:09:21.859720Z",
     "shell.execute_reply": "2024-08-22T16:09:21.859720Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(sp=dict(times=(0,20, 500)), track='none')\n",
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:21.859720Z",
     "iopub.status.busy": "2024-08-22T16:09:21.859720Z",
     "iopub.status.idle": "2024-08-22T16:09:21.984822Z",
     "shell.execute_reply": "2024-08-22T16:09:21.984822Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.8, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Normal Faults, Staged, Long Simulation, No Model History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, removing the tracking makes the long simulations much take less time than the short simulation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Long Simulation Only Necessary Tracking\n",
    "\n",
    "In practice, it can be necessary to track some states over time. Here we perform the same comparison using the 'valstates' option, which only tracks states which have been defined in the model to be necessary to track (using 'valparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:21.988401Z",
     "iopub.status.busy": "2024-08-22T16:09:21.984822Z",
     "iopub.status.idle": "2024-08-22T16:09:26.136688Z",
     "shell.execute_reply": "2024-08-22T16:09:26.136688Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(sp=dict(times=(0,20, 500))) # see default track for Pump\n",
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:26.136688Z",
     "iopub.status.busy": "2024-08-22T16:09:26.136688Z",
     "iopub.status.idle": "2024-08-22T16:09:26.261915Z",
     "shell.execute_reply": "2024-08-22T16:09:26.261915Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.8, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Normal Faults, Staged, Long Simulation, Only Necessary History\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, only tracking a few variables results in similar computational time no tracking.\n",
    "\n",
    "This is because a major computational performance limitation in this model is not necessarily the model simulation itself, but the generation, update, and passing of the history. So it is often best to only track necessary parameters when possible, rather than the entire model history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Lower Tracking Time Resolution\n",
    "\n",
    "Finally, the number of recorded timesteps can be lowered to lower computational costs while still returning all relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:26.261915Z",
     "iopub.status.busy": "2024-08-22T16:09:26.261915Z",
     "iopub.status.idle": "2024-08-22T16:09:31.288935Z",
     "shell.execute_reply": "2024-08-22T16:09:31.288935Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(sp=dict(times=(0,20, 500), track_times=(\"interval\", 5)), track='all')\n",
    "exectimes = compare_pools(mdl, fs, pools, staged=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:31.288935Z",
     "iopub.status.busy": "2024-08-22T16:09:31.288935Z",
     "iopub.status.idle": "2024-08-22T16:09:31.412239Z",
     "shell.execute_reply": "2024-08-22T16:09:31.412239Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 3),)\n",
    "width = 0.8\n",
    "plt.bar(range(len(exectimes)), list(exectimes.values()), align='center', color=\"blue\", label=\"comparison\")\n",
    "plt.bar(range(len(exectimes_baseline)), list(exectimes_baseline.values()), align='center', color=\"gray\", alpha=0.8, label=\"baseline\")\n",
    "plt.xticks(range(len(exectimes)), list(exectimes.keys()))\n",
    "plt.title(\"Computational Performance - Many Faults, Staged, Normal Simulation, Lower Time Resolution\")\n",
    "plt.ylabel(\"Computational Time (s)\")\n",
    "plt.grid(axis='y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, while lowering time resolution could theoretically lower computational time, it does not significantly change much in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:31.412239Z",
     "iopub.status.busy": "2024-08-22T16:09:31.412239Z",
     "iopub.status.idle": "2024-08-22T16:09:31.443559Z",
     "shell.execute_reply": "2024-08-22T16:09:31.443559Z"
    }
   },
   "outputs": [],
   "source": [
    "terminate_pools(pools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Conclusions:\n",
    "\n",
    "Parallelism can the improve computational performance of a given resilience simulation approach. However, this improvement is dependent on the parameters of the simulation. Generally, the official python `multiprocessing` module seems consistently give the best performance improvement over a single-process execution, although this can change depending on the underlying model and modelling approach. There are additionally reasons you might choose other pools-- `multiprocess` pools may enable more data structures in the model because they extend what can be communicated in and out of threads.\n",
    "\n",
    "In general, one of the major considerations for optimization compuational time is not just the *simulation of the model*, but the *size of the returned data structures*. Minimizing the size of the returned data structures can reduce computational time both by reducing the time of an individual simulation and by reducing the *parallelism overhead* from copying these data structures in and out of parallel threads. However, it is important to recognize that for resilience assessment, one often needs a history of model states (or, at least, states of interest) to properly quantify the dynamic costs (i.e., $\\int C_f(t) dt$). Indeed, in this model, only repair costs were able to be used in the comparison of non-tracked states, because the other dynamic costs required a history of their corresponding flows. Changing the number and size of tracked model states can influence the computational time, but only to a point--while one would expect lowering time-fidelity to have a significant effect, it does not because the overhead is less to do with filling the underlying data structures as it has to do with instantiating and returning them--a far more effective method is to only return the functions/flows which are needed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Computational Cost Reduction via Profiling\n",
    "While parallelism and staged execution are helpful and relatively easy-to-implement methods of computational cost reduction, it can be helpful (especially for more complex models) to see what aspects of the model are taking the most computational time.\n",
    "\n",
    "While staged execution was not explored here, it can make a difference when faults are to be injected near the end of the simulation by making it unnecessary to simulate up to the fault time. However, it is less helpful when model instantiation/copy time is a significant fraction of simulation time.\n",
    "\n",
    "Python's builtin `cProfile` package can ge used to see the relative computational times of different functions/processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:31.443559Z",
     "iopub.status.busy": "2024-08-22T16:09:31.443559Z",
     "iopub.status.idle": "2024-08-22T16:09:31.451716Z",
     "shell.execute_reply": "2024-08-22T16:09:31.451716Z"
    }
   },
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:31.451716Z",
     "iopub.status.busy": "2024-08-22T16:09:31.451716Z",
     "iopub.status.idle": "2024-08-22T16:09:31.563370Z",
     "shell.execute_reply": "2024-08-22T16:09:31.563370Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl=Pump(sp=dict(track='all'))\n",
    "prof = cProfile.run('propagate.nominal(mdl)', sort='tottime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:31.563370Z",
     "iopub.status.busy": "2024-08-22T16:09:31.563370Z",
     "iopub.status.idle": "2024-08-22T16:09:37.577351Z",
     "shell.execute_reply": "2024-08-22T16:09:37.577351Z"
    }
   },
   "outputs": [],
   "source": [
    "prof = cProfile.run('propagate.fault_sample(mdl, fs)', sort='tottime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:37.577351Z",
     "iopub.status.busy": "2024-08-22T16:09:37.577351Z",
     "iopub.status.idle": "2024-08-22T16:09:43.558978Z",
     "shell.execute_reply": "2024-08-22T16:09:43.558978Z"
    }
   },
   "outputs": [],
   "source": [
    "prof = cProfile.run('propagate.fault_sample(mdl, fs)', sort='cumtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T16:09:43.558978Z",
     "iopub.status.busy": "2024-08-22T16:09:43.558978Z",
     "iopub.status.idle": "2024-08-22T16:09:43.580095Z",
     "shell.execute_reply": "2024-08-22T16:09:43.580095Z"
    }
   },
   "outputs": [],
   "source": [
    "prof = cProfile.run('Pump()', sort='tottime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, running this model is not particularly computationally expensive. As a result, the majority of the computational expense is not actually because of the simulation itself, but because of the way the model is simulated:\n",
    "- the majority is spent simulating the model\n",
    "- a certain amount is spent re-initalizing the model at first so that the model object can be re-used without worrying about it being modified by any previous executions \n",
    "- another amount is spent recording the model history, wich can increase or decrease depending on tracking options (note the low number of values tracked in the pump model by default)\n",
    "\n",
    "\n",
    "\n",
    "This is mostly because the model itself is computationally inexpensive. However, this example shows how one might easily speed up simulation for optimization or large-n simulations--avoiding unnecessary re-initialization, tracking fewer model states, or speeding up model execution. This can be done in the following ways:\n",
    "- using the options for `track` (as mentioned above) to track fewer states (reducing time spent recording the history)\n",
    "- using `protect` options, which specifies whether the model used is re-instantiated for the simulation (`True`) or used directly (`False`)\n",
    "- speeding up the model by using `dynamic_behavior()` methods instead of `static_behavior()` or `behavior()` methods (which can halve the simulation time at the expense of undirected propagation)\n",
    "- speeding up the model by using a longer global timestep (`'tstep'` in `modelparams`) or by speeding up paricularly expensive Function dynamic behaviors by setting (`dt=local_tstep`) in the `SimParam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
